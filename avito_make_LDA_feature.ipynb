{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "notebookstart= time.time()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Tf-Idf\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# LDA\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "NFOLDS = 5\n",
    "SEED = 1234\n",
    "VALID = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\"\n",
    "    \n",
    "    \n",
    "def rmse(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power((y - y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Load Stage\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Load Stage\")\n",
    "training = pd.read_csv('/home/stanaya/.kaggle/competitions/avito-demand-prediction/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "traindex = training.index\n",
    "testing = pd.read_csv('/home/stanaya/.kaggle/competitions/avito-demand-prediction/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testing.index\n",
    "\n",
    "ntrain = training.shape[0]\n",
    "ntest = testing.shape[0]\n",
    "\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "y = training.deal_probability.copy()\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "# 学習データとテストデータを統合\n",
    "print(\"Combine Train and Test\")\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "del training, testing\n",
    "gc.collect()\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText Features\")\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\", \"title\"]\n",
    "# 句読点、括弧の数を特徴量にする\n",
    "df['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# タイトルと説明文を正規化する\n",
    "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
    "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))\n",
    "\n",
    "# タイトルと説明文の全単語数とユニークな単語種類、さらにそれらの比を撮ったものを加える\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "df_text = df[[\"title\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_replace_title = {',': ' ', '(':' ', ')': ' ', '.':' '}\n",
    "dic_replace_desc = {',': ' ', '(':' ', ')': ' ', '.':' ', '\\n': '', '\\r': '', '/': ''}\n",
    "\n",
    "def replaceTitleSomeCharSplit(s):\n",
    "    return list(filter(None, s.translate(str.maketrans(dic_replace_title)).split(\" \")))\n",
    "\n",
    "def replaceDescSomeCharSplit(s):\n",
    "    return list(filter(None, s.translate(str.maketrans(dic_replace_desc)).split(\" \")))\n",
    "\n",
    "\n",
    "def replaceTitleSomeChar(s):\n",
    "    return s.translate(str.maketrans(dic_replace_title))\n",
    "\n",
    "def replaceDescSomeChar(s):\n",
    "    return s.translate(str.maketrans(dic_replace_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_text[\"title_split\"] = df_text[\"title\"].map(replaceTitleSomeChar)\n",
    "df_text[\"desc_split\"] = df_text[\"description\"].map(replaceDescSomeChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_split = df_text[['title_split', 'desc_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusTitle = df_text_split['title_split'].as_matrix()\n",
    "corpusDesc = df_text_split['desc_split'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011862\n",
      "2011862\n"
     ]
    }
   ],
   "source": [
    "print(len(corpusTitle))\n",
    "print(len(corpusDesc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Modeling!\n"
     ]
    }
   ],
   "source": [
    "## Topic Model\n",
    "print(\"\\nTopic Modeling!\")\n",
    "NUM_TOPICS = 100\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "\n",
    "#vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "#                            stop_words='russian', lowercase=True)\n",
    "vectorizer = CountVectorizer(\n",
    "            stop_words = russian_stop\n",
    "            #,max_df=10\n",
    "            #,min_df=9\n",
    "            )\n",
    "data_title_vectorized = vectorizer.fit_transform(corpusTitle)\n",
    "data_desc_vectorized = vectorizer.fit_transform(corpusDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782920\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "print(len(features))\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic Model\n",
    "#print(\"\\nTopic Modeling!\")\n",
    "#NUM_TOPICS = 10\n",
    "#russian_stop = set(stopwords.words('russian'))\n",
    "#def get_col(col_name): return lambda x: x[col_name]\n",
    "\n",
    " \n",
    "#vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "#                            stop_words='russian', lowercase=True)\n",
    "#vectorizer = CountVectorizer(\n",
    " #           ngram_range=(1, 2),\n",
    "#            stop_words = russian_stop,\n",
    "            #max_features=7000,\n",
    "  #          preprocessor=get_col('title'))\n",
    "#data_vectorized = vectorizer.fit_transform(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI title Runtime: 2.53 Minutes\n",
      "LSI desc Runtime: 7.42 Minutes\n"
     ]
    }
   ],
   "source": [
    "start_topic=time.time()\n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "#lda_model = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "#lda_Z = lda_model.fit_transform(data_title_vectorized)\n",
    "#end_lda=time.time()\n",
    "#print(\"LDA Runtime: %0.2f Minutes\"%((end_lda - start_topic)/60))\n",
    "#print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Non-Negative Matrix Factorization Model\n",
    "#nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "#nmf_Z = nmf_model.fit_transform(data_title_vectorized)\n",
    "#end_nmf=time.time()\n",
    "#print(\"NMF Runtime: %0.2f Minutes\"%((end_nmf - start_topic)/60))\n",
    "#print(nmf_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Latent Semantic Indexing Model\n",
    "lsi_title_model = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "lsi_title_Z = lsi_title_model.fit_transform(data_title_vectorized)\n",
    "end_title_lsi=time.time()\n",
    "print(\"LSI title Runtime: %0.2f Minutes\"%((end_title_lsi - start_topic)/60))\n",
    "\n",
    "lsi_desc_model = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "lsi_desc_Z = lsi_desc_model.fit_transform(data_desc_vectorized)\n",
    "end_desc_lsi=time.time()\n",
    "print(\"LSI desc Runtime: %0.2f Minutes\"%((end_desc_lsi - start_topic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "LSI norm Runtime: 0.00 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n",
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "LSI norm Runtime: 0.05 Minutes\n",
      "200000\n",
      "LSI norm Runtime: 0.09 Minutes\n",
      "300000\n",
      "LSI norm Runtime: 0.14 Minutes\n",
      "400000\n",
      "LSI norm Runtime: 0.19 Minutes\n",
      "500000\n",
      "LSI norm Runtime: 0.24 Minutes\n",
      "600000\n",
      "LSI norm Runtime: 0.28 Minutes\n",
      "700000\n",
      "LSI norm Runtime: 0.33 Minutes\n",
      "800000\n",
      "LSI norm Runtime: 0.38 Minutes\n",
      "900000\n",
      "LSI norm Runtime: 0.42 Minutes\n",
      "1000000\n",
      "LSI norm Runtime: 0.47 Minutes\n",
      "1100000\n",
      "LSI norm Runtime: 0.51 Minutes\n",
      "1200000\n",
      "LSI norm Runtime: 0.56 Minutes\n",
      "1300000\n",
      "LSI norm Runtime: 0.61 Minutes\n",
      "1400000\n",
      "LSI norm Runtime: 0.66 Minutes\n",
      "1500000\n",
      "LSI norm Runtime: 0.70 Minutes\n",
      "1600000\n",
      "LSI norm Runtime: 0.75 Minutes\n",
      "1700000\n",
      "LSI norm Runtime: 0.79 Minutes\n",
      "1800000\n",
      "LSI norm Runtime: 0.84 Minutes\n",
      "1900000\n",
      "LSI norm Runtime: 0.89 Minutes\n",
      "2000000\n",
      "LSI norm Runtime: 0.94 Minutes\n"
     ]
    }
   ],
   "source": [
    "start_time_norm = time.time()\n",
    "for idx in range(len(lsi_title_Z)):\n",
    "    lsi_title_Z[idx] = lsi_title_Z[idx]/sum(lsi_title_Z[idx])\n",
    "    lsi_desc_Z[idx] = lsi_desc_Z[idx]/sum(lsi_desc_Z[idx])\n",
    "    #arr = np.append(arr, tmp, axis=0)\n",
    "    if idx% 100000 == 0:\n",
    "        print(idx)\n",
    "        print(\"LSI norm Runtime: %0.2f Minutes\"%((time.time() - start_time_norm)/60))\n",
    "        gc.collect()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_title_lsi = []\n",
    "header_desc_lsi = []\n",
    "for ele in range(NUM_TOPICS):\n",
    "    name_title_header = 'topic_title_lsi_' + str(ele)\n",
    "    name_desc_header= 'topic_desc_lsi_' + str(ele)\n",
    "    header_title_lsi.append(name_title_header)\n",
    "    header_desc_lsi.append(name_desc_header)\n",
    "\n",
    "df_title_lsi = pd.DataFrame(lsi_title_Z, columns=header_title_lsi)\n",
    "df_desc_lsi = pd.DataFrame(lsi_desc_Z, columns=header_desc_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_split = pd.concat([df_text_split,df_title_lsi], axis=1, join_axes=[df_text_split.index])\n",
    "df_text_split = pd.concat([df_text_split,df_desc_lsi], axis=1, join_axes=[df_text_split.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_split = df_text_split.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_split.drop(\"index\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title_split</th>\n",
       "      <th>desc_split</th>\n",
       "      <th>topic_title_lsi_0</th>\n",
       "      <th>topic_title_lsi_1</th>\n",
       "      <th>topic_title_lsi_2</th>\n",
       "      <th>topic_title_lsi_3</th>\n",
       "      <th>topic_title_lsi_4</th>\n",
       "      <th>topic_title_lsi_5</th>\n",
       "      <th>topic_title_lsi_6</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_desc_lsi_90</th>\n",
       "      <th>topic_desc_lsi_91</th>\n",
       "      <th>topic_desc_lsi_92</th>\n",
       "      <th>topic_desc_lsi_93</th>\n",
       "      <th>topic_desc_lsi_94</th>\n",
       "      <th>topic_desc_lsi_95</th>\n",
       "      <th>topic_desc_lsi_96</th>\n",
       "      <th>topic_desc_lsi_97</th>\n",
       "      <th>topic_desc_lsi_98</th>\n",
       "      <th>topic_desc_lsi_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>кокоби кокон для сна</td>\n",
       "      <td>кокон для сна малыша пользовались меньше месяц...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.018664</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>8.364305e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320578</td>\n",
       "      <td>0.092686</td>\n",
       "      <td>-0.012933</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.095726</td>\n",
       "      <td>-0.039784</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>-0.008363</td>\n",
       "      <td>0.102374</td>\n",
       "      <td>0.159177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>стойка для одежды</td>\n",
       "      <td>стойка для одежды  под вешалки  с бутика</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.010141</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>7.666534e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290425</td>\n",
       "      <td>-0.139372</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.647477</td>\n",
       "      <td>-0.158279</td>\n",
       "      <td>0.187638</td>\n",
       "      <td>0.399396</td>\n",
       "      <td>0.044120</td>\n",
       "      <td>-0.271520</td>\n",
       "      <td>0.231213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>philips bluray</td>\n",
       "      <td>в хорошем состоянии  домашний кинотеатр с blu ...</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.021709</td>\n",
       "      <td>-0.005156</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>4.733695e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096071</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.037947</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.044987</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.027052</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.094011</td>\n",
       "      <td>0.045286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02996f1dd2ea</td>\n",
       "      <td>автокресло</td>\n",
       "      <td>продам кресло от0-25кг</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.029797</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>3.410935e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117723</td>\n",
       "      <td>-0.074474</td>\n",
       "      <td>0.146150</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.104730</td>\n",
       "      <td>0.289469</td>\n",
       "      <td>0.080070</td>\n",
       "      <td>-0.104985</td>\n",
       "      <td>-0.011115</td>\n",
       "      <td>0.075279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7c90be56d2ab</td>\n",
       "      <td>ваз 2110  2003</td>\n",
       "      <td>все вопросы по телефону</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>4.392105e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060620</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.131563</td>\n",
       "      <td>0.044364</td>\n",
       "      <td>0.046470</td>\n",
       "      <td>0.065159</td>\n",
       "      <td>-0.030323</td>\n",
       "      <td>0.033243</td>\n",
       "      <td>0.170228</td>\n",
       "      <td>-0.031962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id            title_split  \\\n",
       "0  b912c3c6a6ad  кокоби кокон для сна    \n",
       "1  2dac0150717d      стойка для одежды   \n",
       "2  ba83aefab5dc         philips bluray   \n",
       "3  02996f1dd2ea             автокресло   \n",
       "4  7c90be56d2ab         ваз 2110  2003   \n",
       "\n",
       "                                          desc_split  topic_title_lsi_0  \\\n",
       "0  кокон для сна малыша пользовались меньше месяц...           0.000041   \n",
       "1          стойка для одежды  под вешалки  с бутика            0.000144   \n",
       "2  в хорошем состоянии  домашний кинотеатр с blu ...           0.000426   \n",
       "3                             продам кресло от0-25кг           0.000409   \n",
       "4                           все вопросы по телефону            0.000004   \n",
       "\n",
       "   topic_title_lsi_1  topic_title_lsi_2  topic_title_lsi_3  topic_title_lsi_4  \\\n",
       "0           0.000135           0.018664          -0.002817           0.006994   \n",
       "1           0.000248           0.010141          -0.000449           0.006938   \n",
       "2           0.000846           0.021709          -0.005156           0.001238   \n",
       "3           0.000637           0.029797          -0.002182          -0.000220   \n",
       "4           0.000009           0.000223          -0.000057           0.000009   \n",
       "\n",
       "   topic_title_lsi_5  topic_title_lsi_6        ...          topic_desc_lsi_90  \\\n",
       "0           0.000175       8.364305e-03        ...                  -0.320578   \n",
       "1           0.000374       7.666534e-03        ...                   0.290425   \n",
       "2           0.001510       4.733695e-03        ...                  -0.096071   \n",
       "3           0.001087       3.410935e-03        ...                   0.117723   \n",
       "4           0.000056       4.392105e-07        ...                  -0.060620   \n",
       "\n",
       "   topic_desc_lsi_91  topic_desc_lsi_92  topic_desc_lsi_93  topic_desc_lsi_94  \\\n",
       "0           0.092686          -0.012933           0.006781           0.095726   \n",
       "1          -0.139372           0.321101           0.647477          -0.158279   \n",
       "2           0.001399          -0.037947           0.019197           0.044987   \n",
       "3          -0.074474           0.146150           0.003283           0.104730   \n",
       "4           0.108844           0.131563           0.044364           0.046470   \n",
       "\n",
       "   topic_desc_lsi_95  topic_desc_lsi_96  topic_desc_lsi_97  topic_desc_lsi_98  \\\n",
       "0          -0.039784           0.024395          -0.008363           0.102374   \n",
       "1           0.187638           0.399396           0.044120          -0.271520   \n",
       "2          -0.038936          -0.027052           0.044617           0.094011   \n",
       "3           0.289469           0.080070          -0.104985          -0.011115   \n",
       "4           0.065159          -0.030323           0.033243           0.170228   \n",
       "\n",
       "   topic_desc_lsi_99  \n",
       "0           0.159177  \n",
       "1           0.231213  \n",
       "2           0.045286  \n",
       "3           0.075279  \n",
       "4          -0.031962  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_split.drop([\"title_split\", \"desc_split\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_split.to_csv(\"topic_feature_lsi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
