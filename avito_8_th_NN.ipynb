{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle kernel の解読"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人のコードのコピー　(https://www.kaggle.com/danieleewww/avito-lightgbm-with-ridge-feature-v-2-0-e0760b/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/stanaya/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Initially forked from Bojan's kernel here: https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2242/code\n",
    "#improvement using kernel from Nick Brook's kernel here: https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm\n",
    "#Used oof method from Faron's kernel here: https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\n",
    "#Used some text cleaning method from Muhammad Alfiansyah's kernel here: https://www.kaggle.com/muhammadalfiansyah/push-the-lgbm-v19\n",
    "import time\n",
    "notebookstart= time.time()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Tf-Idf\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "NFOLDS = 5\n",
    "SEED = 2018\n",
    "VALID = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scikit-learn classifierのラッパークラス\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n",
    "        if(seed_bool == True):\n",
    "            params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF : Out Of Fold\n",
    "def get_oof(clf, x_train, y, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print('\\nFold {}'.format(i))\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\"\n",
    "    \n",
    "    \n",
    "def rmse(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power((y - y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Load Stage\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Load Stage\")\n",
    "training = pd.read_csv('/home/stanaya/.kaggle/competitions/avito-demand-prediction/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "traindex = training.index\n",
    "testing = pd.read_csv('/home/stanaya/.kaggle/competitions/avito-demand-prediction/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testing.index\n",
    "\n",
    "ntrain = training.shape[0]\n",
    "ntest = testing.shape[0]\n",
    "\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "y = training.deal_probability.copy()\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 16 Columns\n"
     ]
    }
   ],
   "source": [
    "# 学習データとテストデータを統合\n",
    "print(\"Combine Train and Test\")\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "#del training, testing\n",
    "gc.collect()\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering\")\n",
    "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "df[\"price\"].fillna(df.price.mean(),inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create Time Variables\n"
     ]
    }
   ],
   "source": [
    "# 曜日、週の情報を追加\n",
    "print(\"\\nCreate Time Variables\")\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "#df[\"Weekd of Year\"] = df['activation_date'].dt.week\n",
    "#df[\"Day of Month\"] = df['activation_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encode Variables\n",
      "Encoding : ['user_id', 'region', 'city', 'parent_category_name', 'category_name', 'user_type', 'image_top_1', 'param_1', 'param_2', 'param_3']\n"
     ]
    }
   ],
   "source": [
    "# Create Validation Index and Remove Dead Variables\n",
    "# training と validation のindexを抽出(item_id)\n",
    "training_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "validation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True) # imageの情報は使わない\n",
    "\n",
    "# categoricalな特徴量を抽出する\n",
    "print(\"\\nEncode Variables\")\n",
    "categorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\",\"param_1\",\"param_2\",\"param_3\"]\n",
    "print(\"Encoding :\",categorical)\n",
    "\n",
    "# Encoder:\n",
    "# 文字データを離散数値に変換\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col].fillna('Unknown')\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText Features\")\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\", \"title\"]\n",
    "# 句読点、括弧の数を特徴量にする\n",
    "df['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# タイトルと説明文を正規化する\n",
    "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
    "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))\n",
    "\n",
    "# タイトルと説明文の全単語数とユニークな単語種類、さらにそれらの比を撮ったものを加える\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    df[cols + '_num_letters'] = df[cols].apply(lambda comment: len(comment)) # Count number of Letters\n",
    "    df[cols + '_num_alphabets'] = df[cols].apply(lambda comment: (comment.count(r'[a-zA-Z]'))) # Count number of Alphabets\n",
    "    df[cols + '_num_alphanumeric'] = df[cols].apply(lambda comment: (comment.count(r'[A-Za-z0-9]'))) # Count number of AlphaNumeric\n",
    "    df[cols + '_num_digits'] = df[cols].apply(lambda comment: (comment.count('[0-9]'))) # Count number of Digits\n",
    "    \n",
    "df['title_desc_len_ratio'] = df['title_num_letters']/df['description_num_letters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TF-IDF] Term Frequency Inverse Document Frequency Stage\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\n",
    "# ロシア語のストップワード（自然言語処理する際に一般的で役に立たない等の理由で処理対象外とする単語）の除去\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "##I added to the max_features of the description. It did not change my score much but it may be worth investigating\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=17000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('title',CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Runtime: 16.13 Minutes\n"
     ]
    }
   ],
   "source": [
    "start_vect=time.time()\n",
    "\n",
    "#Fit my vectorizer on the entire dataset instead of the training rows\n",
    "#Score improved by .0001\n",
    "vectorizer.fit(df.to_dict('records'))\n",
    "\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Text Cols\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df.drop(textfeats, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Fold 2\n",
      "\n",
      "Fold 3\n",
      "\n",
      "Fold 4\n",
      "Ridge OOF RMSE: 0.23033890453576603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "ridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
    "                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n",
    "\n",
    "#Ridge oof method from Faron's kernel\n",
    "#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n",
    "#It doesn't really add much to the score, but it does help lightgbm converge faster\n",
    "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, ridge_oof_train))\n",
    "print('Ridge OOF RMSE: {}'.format(rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling Stage\n"
     ]
    }
   ],
   "source": [
    "print(\"Modeling Stage\")\n",
    "\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "\n",
    "df['ridge_preds'] = ridge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>...</th>\n",
       "      <th>description_num_digits</th>\n",
       "      <th>title_num_words</th>\n",
       "      <th>title_num_unique_words</th>\n",
       "      <th>title_words_vs_unique</th>\n",
       "      <th>title_num_letters</th>\n",
       "      <th>title_num_alphabets</th>\n",
       "      <th>title_num_alphanumeric</th>\n",
       "      <th>title_num_digits</th>\n",
       "      <th>title_desc_len_ratio</th>\n",
       "      <th>ridge_preds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b912c3c6a6ad</th>\n",
       "      <td>884270</td>\n",
       "      <td>19</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>249</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>5.991467</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.111301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dac0150717d</th>\n",
       "      <td>227908</td>\n",
       "      <td>17</td>\n",
       "      <td>1314</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>8.006368</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.210374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba83aefab5dc</th>\n",
       "      <td>576261</td>\n",
       "      <td>16</td>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>8.294050</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.179773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02996f1dd2ea</th>\n",
       "      <td>755087</td>\n",
       "      <td>21</td>\n",
       "      <td>950</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>112</td>\n",
       "      <td>1217</td>\n",
       "      <td>7.696213</td>\n",
       "      <td>286</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.308327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c90be56d2ab</th>\n",
       "      <td>944363</td>\n",
       "      <td>4</td>\n",
       "      <td>318</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>124</td>\n",
       "      <td>46</td>\n",
       "      <td>10.596635</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.360751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id  region  city  parent_category_name  category_name  \\\n",
       "item_id                                                                    \n",
       "b912c3c6a6ad   884270      19   462                     4             42   \n",
       "2dac0150717d   227908      17  1314                     2             22   \n",
       "ba83aefab5dc   576261      16  1290                     0              2   \n",
       "02996f1dd2ea   755087      21   950                     4             42   \n",
       "7c90be56d2ab   944363       4   318                     6              0   \n",
       "\n",
       "              param_1  param_2  param_3      price  item_seq_number  \\\n",
       "item_id                                                               \n",
       "b912c3c6a6ad      249      112     1217   5.991467                2   \n",
       "2dac0150717d      122      112     1217   8.006368               19   \n",
       "ba83aefab5dc       84      112     1217   8.294050                9   \n",
       "02996f1dd2ea       38      112     1217   7.696213              286   \n",
       "7c90be56d2ab      278      124       46  10.596635                3   \n",
       "\n",
       "                 ...       description_num_digits  title_num_words  \\\n",
       "item_id          ...                                                 \n",
       "b912c3c6a6ad     ...                            0                3   \n",
       "2dac0150717d     ...                            0                3   \n",
       "ba83aefab5dc     ...                            0                2   \n",
       "02996f1dd2ea     ...                            0                1   \n",
       "7c90be56d2ab     ...                            0                3   \n",
       "\n",
       "              title_num_unique_words  title_words_vs_unique  \\\n",
       "item_id                                                       \n",
       "b912c3c6a6ad                       3                  100.0   \n",
       "2dac0150717d                       3                  100.0   \n",
       "ba83aefab5dc                       2                  100.0   \n",
       "02996f1dd2ea                       1                  100.0   \n",
       "7c90be56d2ab                       3                  100.0   \n",
       "\n",
       "              title_num_letters  title_num_alphabets  title_num_alphanumeric  \\\n",
       "item_id                                                                        \n",
       "b912c3c6a6ad                 21                    0                       0   \n",
       "2dac0150717d                 17                    0                       0   \n",
       "ba83aefab5dc                 14                    0                       0   \n",
       "02996f1dd2ea                 10                    0                       0   \n",
       "7c90be56d2ab                 14                    0                       0   \n",
       "\n",
       "              title_num_digits  title_desc_len_ratio  ridge_preds  \n",
       "item_id                                                            \n",
       "b912c3c6a6ad                 0              0.362069     0.111301  \n",
       "2dac0150717d                 0              0.414634     0.210374  \n",
       "ba83aefab5dc                 0              0.141414     0.179773  \n",
       "02996f1dd2ea                 0              0.454545     0.308327  \n",
       "7c90be56d2ab                 0              0.583333     0.360751  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Text Cols\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df.drop(textfeats, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LSI feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsi_title_raw = pd.read_csv('topic_feature_lsi_title_100_raw.csv', index_col = \"item_id\")\n",
    "df_lsi_desc_raw = pd.read_csv('topic_feature_lsi_desc_100_raw.csv', index_col = \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsi_title_raw.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_lsi_desc_raw.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lsi_feature_raw = pd.concat([df_lsi_title_raw,df_lsi_desc_raw], axis=1, join_axes=[df_lsi_desc_raw.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lsi_feature_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsi_title_std = pd.read_csv('topic_feature_lsi_title_100_std.csv', index_col = \"item_id\")\n",
    "df_lsi_desc_std= pd.read_csv('topic_feature_lsi_desc_100_std.csv', index_col = \"item_id\")\n",
    "df_lsi_title_std.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_lsi_desc_std.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lsi_feature_std = pd.concat([df_lsi_title_std, df_lsi_desc_std], axis=1, join_axes=[df_lsi_desc_std.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsi_feature_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NMF feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf_title_raw = pd.read_csv('topic_feature_nmf_title_100_raw.csv', index_col = \"item_id\")\n",
    "df_nmf_desc_raw = pd.read_csv('topic_feature_nmf_desc_100_raw.csv', index_col = \"item_id\")\n",
    "df_nmf_title_raw.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_nmf_desc_raw.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf_feature_raw = pd.concat([df_nmf_title_raw,df_nmf_desc_raw], axis=1, join_axes=[df_nmf_desc_raw.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nmf_feature_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.head()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix_feature_nmf_lsi = pd.concat([df_nmf_title_raw,df_nmf_desc_raw, df_lsi_desc_std], axis=1, join_axes=[df_nmf_desc_raw.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix_feature_nmf_lsi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn_input = pd.concat([df, df_nmf_feature_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "X = df.loc[traindex,:].values df_nmf_desc_raw.loc[traindex,:].values\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:], csr_matrix(df_nmf_desc_raw.loc[testdex,:].values)])\n",
    "tfvocab = df.columns.tolist() + tfvocab  + df_nmf_desc_raw.columns.tolist()\n",
    "\n",
    "for shape in [X,testing]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import * \n",
    "rand_seed = randint(2147483647)\n",
    "print(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN on Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras,optimizers import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Evaluation Stage\")\n",
    "lgpred = lgb_clf.predict(testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name =  \"lgb_model_ver2_\" + \"nmf\" + \"_100_\" + \"seed_\" + str(rand_seed) + \".txt\"\n",
    "lgb_clf.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mixing lightgbm with ridge. I haven't really tested if this improves the score or not\n",
    "#blend = 0.95*lgpred + 0.05*ridge_oof_test[:,0]\n",
    "lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "lgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "output_file_name = \"lgbsub_ver2_\" + \"nmf\" + \"_100_\" + \"seed_\" + str(rand_seed) + \".csv\"\n",
    "lgsub.to_csv(output_file_name,index=True,header=True)\n",
    "#print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
